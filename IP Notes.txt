CONVOLUTION MATRIX: the concept is amazingly simple and intuitive... check this out first: https://en.wikipedia.org/wiki/Kernel_(image_processing)
It is a characteristic of a transformation... So the desired effect can be obtained by applying the concept.

LOCAL BINARY PATTERNS: The pattern obtained by applying the following transfomation: obtain an 8-bit number for each pixel - formed by assigning 1/0 to each adjacent pixel based on whether it is more/less intense than the center pixel. This plot preserves RELATIVE pattern and independent of any Offset etc. refer: https://en.wikipedia.org/wiki/Local_binary_patterns  http://www.scholarpedia.org/article/Local_Binary_Patterns
It gives the pattern/texture and hence the reqd. underlying data about color-patterns.. 

EDGE-DETECTION: Canny-edge; refer to the PDFs in the folder by the same name. Form gradient by a convolution matrix etc. Thresholding and then joining the edges based on approx. angles of gradient and forming major and minor edges etc. Done fairly qualitatively for now.. Further when actually programming...

MEAN-SHIFT ALGORITHM: So... The aplication of the algorithm (on the front-end) is similar to the use in Laplace's Solution. To understand more refer the Wiki PDF, highlighted o/w. So we have the converging iteration (convergence proved, but bleh.) and hence the method allows us to find and enhance MODES. Great stress is laid to kernels(convolution matrix) and basically we're looking at 3 kinds.
Note that the  above mentioned is only the application step and actual processing is quite justified. As noticed in the edge-detection algorithm, NOISE must be brought down to produce efficient results in edge-detection. Here comes the role of Mean-shift. (Other applications include clustering, filtering, camshift etc.)
Now, looking at the basic purpose of reducing noise and clustering modes etc. there comes a parameter called resolution 'h'. NOT SURE HOW AND WHY but larger 'h' implies faster convergence and hence the modes may converge and more details lost. (One reasoning: the frequency distribution -histogram style- has a term of (del.X/h), thus a smaller 'h' ensures a more accurate del.X, or X-Xi(to keep distribution same/finite, they'd vary accordingly) and hence a small 'h' ensures slower convergence and retention of more features/noise) (refer Diagrams on Slide nos. 14 and 16)
- Comparing with other clustering algo K-means (to be taken up soon), it does not ASSUME any cluster etc. and the algo ensures that clusters are sorted automatically. Also, it is robust and works for any no. of (non-predefined) modes.
-However, the iterative technique is highly redundant and computationally expensive; also, the method doesnt work well in free space (3D), as there may exist many local maximas that converge to optimas and mode isolation cannot be done.
